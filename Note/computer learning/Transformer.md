# Can machines think ？ 

图灵测试
		具体来说，图灵测试的规则如下：
	1. 评委通过一种纯文本对话与两个参与者交流。
	2. 一个参与者是人类（通常被称为"人类评委"），另一个是计算机程序，它可以表现为人类或回答问题。
	如果评委不能准确区分哪一个参与者是计算机，哪一个是人类，那么计算机被认为通过了图灵测试。换句话说，计算机成功地模拟了人类的对话，表现出了人类级别的智能。

那么让机器理解人类语言NLP就是第一步， 这是人工智能发展中至关重要的一环。

# 自然语言处理（NLP）

## 1.规则定义模型
* 研究员手动定义规则。机器从库中检索问题，回答对应的答案。

## 2.通用模型
* 基于统计方法的模型，基于马尔可夫假设。（一个词语出现的概率，只和前面的n，n+1个词语有关，而与后面的词语都无关。）

### n元模型：
	一个词语出现的概率，之和n-1个词语有关。
	但随着n的增大，所需要记录的概率分布呈指数级增长，这就意味着我们并不能拥有一个长的上下文。（长距离依赖问题） 模型就会变得低效

## 基于神经网络的NLP模型
(神经网络，启发于人脑的工作逻辑)
	
### RNN（recurrent neural network）
* 人脑中的神经元，通过循环的方式相互连接。Hebbian（同激活的神经元会共连接）
* 人脑中，一个神经元的输出信号，是其他神经元的输入信号。
* RNN模型受人脑启发，在序列化数据处理方面显著成效。缓解了n元模型长距离依赖的问题。
	
* 问题：梯度消失
	因为有激活函数的存在，在反向传播时，对应值较小的神经元很容易被忽视掉。而改变值更大的神经元的权重，较小的修正，就能带来更大的改变。

## Transformer模型
### Word embedding
词语去向量化，将单词映射为高纬向量，使得单词之间的语义关系可以在向量空间中体现。


## Attention机制

大脑的运作逻辑，是否就是注意力的转移和集中？

## self-Attention
自注意力机制的核心思想是，模型可以同时考虑输入序列（如文本或图像）中不同位置的元素，并为每个位置分配不同的注意权重，以便更好地捕捉元素之间的关系。自注意力的关键优势是在处理长序列时能够维持较远距离的依赖性，而不需要像循环神经网络（RNN）或卷积神经网络（CNN）那样限制距离。


## Multi-head注意力
多头注意力的核心思想是将自注意力机制（Self-Attention）应用多次，每次都学习不同的权重分配，以捕捉输入序列中不同方面的关系。每个注意力头产生一个不同的输出，这些输出会被拼接在一起，然后经过一个线性变换，以生成最终的多头注意力输出。

Transformer模型是一种用于处理序列数据的神经网络架构，主要用于自然语言处理（NLP）任务，例如文本翻译、文本生成和语义理解。Transformer模型由编码器（Encoder）和解码器（Decoder）两部分组成，它们协同工作以实现各种序列到序列的任务。下面分别介绍编码器和解码器的功能和组成。


## 编码器/解码器

**编码器（Encoder）**:
1. **输入序列的编码**：编码器接受输入序列（例如源语言句子）作为输入，然后将其转换成一系列表示。这些表示包含了输入序列中每个位置的信息，包括词嵌入（word embeddings）和位置嵌入（position embeddings）。

2. **多层自注意力和前馈神经网络**：编码器由多个相同的层组成，每个层都包括两个主要部分：多头自注意力（Multi-head Self-Attention）和前馈神经网络（Feedforward Neural Network）。这些层有助于编码输入序列的信息，并捕捉不同单词之间的关系。

3. **残差连接和层归一化**：每个子层都包括残差连接和层归一化，这有助于避免梯度消失问题，使模型更容易训练。

4. **编码器堆叠**：编码器可以由多个这样的层堆叠而成，增加了模型的表示能力。

**解码器（Decoder）**:
1. **目标序列的生成**：解码器接受编码器的输出表示（通常称为编码器输出或上下文表示），并用于生成目标序列（例如目标语言句子）。

2. **多层自注意力、编码器-解码器注意力和前馈神经网络**：解码器也由多个相同的层组成，每个层包括多头自注意力、编码器-解码器注意力（用于关联输入和输出序列的信息）以及前馈神经网络。

3. **残差连接和层归一化**：与编码器一样，解码器的每个子层都包括残差连接和层归一化。

4. **解码器堆叠**：解码器也可以由多个这样的层堆叠而成，以增加模型的表示能力。

在机器翻译任务中，例如从英语翻译成法语，编码器处理英语输入句子，解码器生成法语输出句子。编码器和解码器之间的信息流通过编码器-解码器注意力层来实现。这种分层结构和自注意力机制的组合使得Transformer能够有效地处理各种NLP任务，而无需依赖传统的循环神经网络或卷积神经网络。![[computer learning/截屏2023-10-25 13.43.35.png]]